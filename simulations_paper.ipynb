{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "521bf589",
            "metadata": {},
            "source": [
                "# Gaussian Field Simulation Pipeline\n",
                "\n",
                "Generate the synthetic Gaussian random fields used throughout the anisotropy study. The notebook configures the gstools covariance model, samples multiple realizations per design point, extracts contour-level measurements, and saves the resulting statistics for later analysis in `notebook_paper.ipynb`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "34a24086",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core dependencies for the simulation workflow.\n",
                "from __future__ import print_function\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import gstools as gs\n",
                "import seaborn as sns\n",
                "import matplotlib.lines as mlines  # Supports bespoke legend entries\n",
                "import pandas as pd\n",
                "from scipy.integrate import quad\n",
                "from scipy.optimize import brentq\n",
                "from scipy.stats import norm, chi2\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
                "from skimage.measure import euler_number, label\n",
                "from tqdm.auto import tqdm \n",
                "# np.random.seed(31415926)  # Uncomment for deterministic experiments.\n",
                "\n",
                "# Align Matplotlib/Seaborn styling with the analysis notebook.\n",
                "sns.set_theme(context='paper', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n",
                "sns.set_style()\n",
                "\n",
                "# Helper: retrieve contour geometry (perimeters, normals, equispaced points) without rendering figures.\n",
                "def get_contour_properties(gsfield, level=0, total_target_points=int(1e7)):\n",
                "    \"\"\"\n",
                "    Return normals, perimeters, equispaced points, and allocation counts for the contours of `gsfield` at `level`.\n",
                "    Points are apportioned proportionally to contour lengths so that ∑ points ≈ total_target_points.\n",
                "    \"\"\"\n",
                "    # Rely on Matplotlib to extract contour segments without rendering a figure.\n",
                "    cs = plt.contour(gsfield, levels=[level])\n",
                "    all_segments_paths = []\n",
                "    if cs.allsegs and cs.allsegs[0]:\n",
                "        all_segments_paths = cs.allsegs[0]\n",
                "    plt.close()  # Close the figure to prevent leaking handles.\n",
                "\n",
                "    all_normals = []\n",
                "    equispaced_points_coords = []\n",
                "    total_perimeter = 0\n",
                "    points_per_path_actual = [] \n",
                "\n",
                "    if not all_segments_paths:\n",
                "        return [], 0, [], []\n",
                "\n",
                "    # Measure each path length to distribute points proportionally.\n",
                "    path_lengths = []\n",
                "    for path in all_segments_paths:\n",
                "        if len(path) < 2:\n",
                "            path_lengths.append(0)\n",
                "            continue\n",
                "        lengths = np.sqrt(np.sum(np.diff(path, axis=0)**2, axis=1))\n",
                "        current_path_length = np.sum(lengths)\n",
                "        path_lengths.append(current_path_length)\n",
                "        total_perimeter += current_path_length\n",
                "\n",
                "    if total_perimeter == 0:\n",
                "        points_per_path_actual = [0] * len(all_segments_paths)\n",
                "        return [], 0, [], points_per_path_actual\n",
                "    \n",
                "    # Ideal (non-integer) allocation of points per path based on relative length.\n",
                "    num_points_ideal_float = []\n",
                "    for length in path_lengths:\n",
                "        proportion = length / total_perimeter if total_perimeter > 0 else 0\n",
                "        num_points_ideal_float.append(proportion * total_target_points)\n",
                "\n",
                "    points_per_path_actual = [int(np.floor(n)) for n in num_points_ideal_float]\n",
                "    \n",
                "    # Guarantee that sufficiently long paths get at least two samples when feasible.\n",
                "    for i_path in range(len(points_per_path_actual)):\n",
                "        if num_points_ideal_float[i_path] > 0.5 and points_per_path_actual[i_path] < 2 and len(all_segments_paths[i_path]) >= 2:\n",
                "            points_per_path_actual[i_path] = 2\n",
                "        elif len(all_segments_paths[i_path]) < 2:\n",
                "             points_per_path_actual[i_path] = 0\n",
                "\n",
                "    # Assign leftover points by descending fractional remainder to reach total_target_points.\n",
                "    current_sum_points = sum(points_per_path_actual)\n",
                "    remainder_points = total_target_points - current_sum_points\n",
                "    \n",
                "    if remainder_points > 0:\n",
                "        fractional_parts = [n - np.floor(n) for n in num_points_ideal_float]\n",
                "        sorted_indices = np.argsort(fractional_parts)[::-1]\n",
                "        \n",
                "        for i in range(int(remainder_points)):\n",
                "            path_idx_to_increment = sorted_indices[i % len(sorted_indices)]\n",
                "            if len(all_segments_paths[path_idx_to_increment]) >= 2:\n",
                "                 points_per_path_actual[path_idx_to_increment] += 1\n",
                "\n",
                "    # Clear allocations on degenerate paths (zero length or single vertex).\n",
                "    for i_path in range(len(points_per_path_actual)):\n",
                "        if points_per_path_actual[i_path] > 0 and (len(all_segments_paths[i_path]) < 2 or path_lengths[i_path] == 0):\n",
                "            points_per_path_actual[i_path] = 0\n",
                "\n",
                "    # Generate equispaced samples and normals along each contour.\n",
                "    for i_path, path_coords in enumerate(all_segments_paths):\n",
                "        num_points_this_path = points_per_path_actual[i_path]\n",
                "\n",
                "        if num_points_this_path < 2:\n",
                "            continue  # Skip paths without enough vertices to define normals.\n",
                "        \n",
                "        seg_lengths = np.sqrt(np.sum(np.diff(path_coords, axis=0)**2, axis=1))\n",
                "        cumulative_lengths = np.concatenate(([0], np.cumsum(seg_lengths)))\n",
                "        \n",
                "        if cumulative_lengths[-1] == 0:\n",
                "            continue\n",
                "\n",
                "        target_distances = np.linspace(0, cumulative_lengths[-1], num_points_this_path, endpoint=True) \n",
                "        # When only two points are placed, pin them to the path endpoints for clarity.\n",
                "        if num_points_this_path == 2 and cumulative_lengths[-1] > 0:\n",
                "             target_distances = np.array([0, cumulative_lengths[-1]])\n",
                "\n",
                "        for dist_idx, dist in enumerate(target_distances):\n",
                "            dist = min(dist, cumulative_lengths[-1])\n",
                "            dist = max(dist, 0)\n",
                "\n",
                "            seg_idx = np.searchsorted(cumulative_lengths, dist, side='right') - 1\n",
                "            seg_idx = max(0, seg_idx) \n",
                "            \n",
                "            if seg_idx >= len(seg_lengths): \n",
                "                seg_idx = len(seg_lengths) -1\n",
                "\n",
                "            pt_A = path_coords[seg_idx]\n",
                "            pt_B = path_coords[seg_idx + 1] \n",
                "            \n",
                "            dist_along_segment = dist - cumulative_lengths[seg_idx]\n",
                "            current_segment_length = seg_lengths[seg_idx] if seg_idx < len(seg_lengths) else 0\n",
                "            \n",
                "            if current_segment_length == 0:\n",
                "                if dist >= cumulative_lengths[seg_idx+1] if seg_idx + 1 < len(cumulative_lengths) else cumulative_lengths[-1]:\n",
                "                    x_p, y_p = pt_B[0], pt_B[1]\n",
                "                else:\n",
                "                    x_p, y_p = pt_A[0], pt_A[1]\n",
                "            else:\n",
                "                ratio = dist_along_segment / current_segment_length\n",
                "                x_p = pt_A[0] + ratio * (pt_B[0] - pt_A[0])\n",
                "                y_p = pt_A[1] + ratio * (pt_B[1] - pt_A[1])\n",
                "            \n",
                "            equispaced_points_coords.append(np.array([x_p, y_p]))\n",
                "            \n",
                "            # Normal vector is obtained via the clockwise rotation of the tangent.\n",
                "            dx_tangent = pt_B[0] - pt_A[0]\n",
                "            dy_tangent = pt_B[1] - pt_A[1]\n",
                "            nx = -dy_tangent\n",
                "            ny = dx_tangent\n",
                "            norm_magnitude = np.sqrt(nx**2 + ny**2)\n",
                "            \n",
                "            if norm_magnitude > 1e-9:\n",
                "                nx /= norm_magnitude\n",
                "                ny /= norm_magnitude\n",
                "            else:\n",
                "                if all_normals:\n",
                "                    nx, ny = all_normals[-1]\n",
                "                elif len(path_coords) > seg_idx + 2 :\n",
                "                    pt_C = path_coords[seg_idx + 2]\n",
                "                    dx_tangent = pt_C[0] - pt_B[0]\n",
                "                    dy_tangent = pt_C[1] - pt_B[1]\n",
                "                    nx = -dy_tangent\n",
                "                    ny = dx_tangent\n",
                "                    norm_magnitude = np.sqrt(nx**2 + ny**2)\n",
                "                    if norm_magnitude > 1e-9:\n",
                "                        nx /= norm_magnitude\n",
                "                        ny /= norm_magnitude\n",
                "                    else:\n",
                "                        nx, ny = 0, 0\n",
                "                else:\n",
                "                    nx, ny = 0, 0\n",
                "            all_normals.append((nx, ny))\n",
                "\n",
                "    return all_normals, total_perimeter, equispaced_points_coords, points_per_path_actual"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c9c06cb4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper: gradient computation expressed in physical units\n",
                "def compute_gradient_field(gsfield: np.ndarray, spacing: float) -> tuple[np.ndarray, np.ndarray]:\n",
                "    \"\"\"Return ∂Z/∂x and ∂Z/∂y arrays scaled by the physical spacing between pixels.\n",
                "\n",
                "    Notes:\n",
                "    - spacing equals gstools_model_rescale_factor (= 1 / unit_length) along both axes.\n",
                "    - np.gradient yields (dZ/dy, dZ/dx) for 2D arrays, so swap the outputs before returning.\n",
                "    - edge_order=2 applies a higher-accuracy centered stencil.\n",
                "    \"\"\"\n",
                "    if not isinstance(gsfield, np.ndarray) or gsfield.ndim != 2:\n",
                "        raise ValueError(\"gsfield must be a 2D numpy array\")\n",
                "    gy, gx = np.gradient(gsfield, spacing, spacing, edge_order=2)\n",
                "    return gx, gy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "179328b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model parameters and geometric configuration\n",
                "kappa = 0.5 # Kappa parameter (0 < kappa < 1) driving anisotropy\n",
                "a_parameter = (1 / (1-kappa**2))**(1/4) # Rescaling factor derived from kappa\n",
                "print(\"kappa = \", kappa)\n",
                "print(\"a_parameter = \", a_parameter)\n",
                "\n",
                "field_size = 1000 # Grid size (pixels per axis)\n",
                "unit_length = 5 # Physical unit spans this many pixels along each axis\n",
                "T_parameter = field_size / unit_length # Observation window size expressed in physical units\n",
                "print(\"T_parameter = \", T_parameter)\n",
                "\n",
                "gstools_model_rescale_factor = 1/unit_length # Spatial rescaling applied to each coordinate\n",
                "gstools_model_len_scale = np.sqrt(2) * np.array([a_parameter, 1/a_parameter]) # Principal correlation lengths of the covariance model\n",
                "# Rotation applied to the covariance matrix\n",
                "gstools_model_anis_ratio = a_parameter**(-2) # Anisotropy ratio parameter passed to gstools\n",
                "angle_rad = 1 # Rotation angle (radians)\n",
                "print(\"theta = \", angle_rad)\n",
                "\n",
                "level_val = 1   # Level set under study\n",
                "print(\"level_val = \", level_val)\n",
                "\n",
                "min_length = unit_length # Reject contours shorter than one physical unit\n",
                "\n",
                "# Simulation campaign controls\n",
                "loop_size = int(3e2) # Total Monte Carlo realizations\n",
                "nb_points_level_set = int(1e7) # Target number of sampled points along all contours\n",
                "print(\"nb_points_level_set = \", nb_points_level_set)\n",
                "print(\"loop_size = \", loop_size)\n",
                "\n",
                "factor_divide = 25 # Grid subdivision used when aggregating Cabana statistics\n",
                "print(\"factor_divide = \", factor_divide)\n",
                "\n",
                "\n",
                "# DataFrame schema for accumulating experiment outputs\n",
                "results_df = pd.DataFrame(columns=[\n",
                "    'kappa', \n",
                "    'angle_rad',\n",
                "    'a_parameter',\n",
                "    'field_size', \n",
                "    'unit_length', \n",
                "    'T_parameter', \n",
                "    'gstools_model_anis_ratio', \n",
                "    'gstools_model_rescale_factor', \n",
                "    'gstools_model_len_scale', \n",
                "    'level_val', \n",
                "    'total_points_generated',\n",
                "    'sigma_emp_xx', 'sigma_emp_xy', 'sigma_emp_yy',\n",
                "    'estimated_angle_cabana',\n",
                "    'estimated_kappa_cabana', \n",
                "    'empirical_cosine_cabana', \n",
                "    'empirical_sine_cabana',\n",
                "    'euler_characteristic', \n",
                "    'perimeter', \n",
                "    'volume_sublevel_set', \n",
                "    'perimeter_cabana', \n",
                "    'estimated_kappa_BD',\n",
                "    'statistics_BD',\n",
                "    'mean_statistics_BD',\n",
                "    'euler_characteristic_scikit_1',\n",
                "    'euler_characteristic_scikit_2',\n",
                "    'estimated_kappa_BD_scikit_1',\n",
                "    'estimated_kappa_BD_scikit_2',\n",
                "    'chi2_statistic',\n",
                "    'chi2_pvalue',\n",
                "    'factor_divide',\n",
                "    'full_covariance_emp_gradients_xx',\n",
                "    'full_covariance_emp_gradients_xy',\n",
                "    'full_covariance_emp_gradients_yy',\n",
                "    'estimated_kappa_gradients',\n",
                "    'estimated_angle_gradients',\n",
                "    'full_correlation_emp_gradients_xx',\n",
                "    'full_correlation_emp_gradients_xy',\n",
                "    'full_correlation_emp_gradients_yy',\n",
                "    'estimated_kappa_normals',\n",
                "    'estimated_angle_normals'\n",
                "])\n",
                "\n",
                "# Main simulation loop running gstools sampling, contour extraction, and summary statistics\n",
                "for i in tqdm(range(loop_size), desc=\"Simulation Progress\"):\n",
                "\n",
                "    # 1) Generate a Gaussian field and compute Euler characteristics via scikit-image\n",
                "    # Draw a Gaussian random field with gstools\n",
                "    # Configure the anisotropic Gaussian model and rotation\n",
                "    x_coords = y_coords = range(field_size)\n",
                "    model = gs.Gaussian(dim=2, var=1, len_scale=gstools_model_len_scale, angles=angle_rad, rescale=gstools_model_rescale_factor)\n",
                "    srf = gs.SRF(model) \n",
                "    gsfield = srf((x_coords, y_coords), mesh_type='structured')\n",
                "    # Compute the gradient field for empirical covariance estimates\n",
                "    spacing = float(gstools_model_rescale_factor)  # Equals 1 / unit_length\n",
                "    grad_x, grad_y = compute_gradient_field(gsfield, spacing=spacing)\n",
                "    # Optional: empirical covariance of gradient vectors across the grid\n",
                "    G = np.column_stack((grad_x.ravel(), grad_y.ravel()))\n",
                "    # Drop any NaNs from the gradients (should be rare)\n",
                "    G = G[~np.isnan(G).any(axis=1)]\n",
                "    norm_G = np.linalg.norm(G, axis=1)\n",
                "    normalized_G = G / norm_G[:, np.newaxis]    \n",
                "    if len(G) > 1:\n",
                "        covG = np.cov(G, rowvar=False, bias=True)\n",
                "        cov_normG = np.cov(normalized_G, rowvar=False, bias=True)\n",
                "        full_covariance_emp_gradients_xx = covG[0, 0]\n",
                "        full_covariance_emp_gradients_xy = covG[0, 1]\n",
                "        full_covariance_emp_gradients_yy = covG[1, 1]\n",
                "        # Compute correlation coefficients for normalized gradients\n",
                "        full_correlation_emp_gradients_xx = cov_normG[0, 0]\n",
                "        full_correlation_emp_gradients_xy = cov_normG[0, 1]\n",
                "        full_correlation_emp_gradients_yy = cov_normG[1, 1]\n",
                "        # Estimate κ and the principal angle from the covariance matrices\n",
                "        trace_covG = full_covariance_emp_gradients_xx + full_covariance_emp_gradients_yy\n",
                "        det_covG = full_covariance_emp_gradients_xx * full_covariance_emp_gradients_yy - full_covariance_emp_gradients_xy**2\n",
                "        if det_covG > 0:\n",
                "            eigenval_cov, eigenvec_cov = np.linalg.eig(covG)\n",
                "            eigenval_norm, eigenvec_norm = np.linalg.eig(cov_normG)\n",
                "            kappa_estimate_gradients = np.sqrt(1 - (eigenval_cov.min() / eigenval_cov.max()))\n",
                "            kappa_estimate_normals = np.sqrt(1 - (eigenval_norm.min() / eigenval_norm.max()))\n",
                "            estimated_kappa_gradients = kappa_estimate_gradients\n",
                "            estimated_kappa_normals = kappa_estimate_normals\n",
                "            # Reference: https://math.stackexchange.com/questions/2733847/how-can-i-calculate-the-angle-of-an-ellipse-given-its-matrix-representation\n",
                "            estimated_angle_gradients = -0.5 * np.arctan2(2 * full_covariance_emp_gradients_xy, full_covariance_emp_gradients_xx - full_covariance_emp_gradients_yy)\n",
                "            estimated_angle_normals = -0.5 * np.arctan2(2 * full_correlation_emp_gradients_xy, full_correlation_emp_gradients_xx - full_correlation_emp_gradients_yy) #This formula gives the angle of the major axis but it is not supported by Palm distribution analysis, we do not use it.\n",
                "    else:\n",
                "        print(\"Not enough points for covariance.\")\n",
                "    \n",
                "    bool_image = (gsfield > level_val)\n",
                "    euler_characteristic_scikit = euler_number(bool_image, connectivity=1)\n",
                "    euler_characteristic_scikit2 = euler_number(bool_image, connectivity=2)\n",
                "\n",
                "    # 2) Generate equispaced points along level-set contours and collect normals/perimeter lengths.\n",
                "    all_normals_level, perimeter, equispaced_points_level, points_per_path_actual = get_contour_properties(gsfield, level=level_val, total_target_points=nb_points_level_set)\n",
                "    perimeter *= gstools_model_rescale_factor # Rescale perimeter to physical units\n",
                "    # Placeholder: insert filtering for short paths if required\n",
                "    \n",
                "    # 3) Compute empirical covariance of surface normals\n",
                "    sigma_emp_xx, sigma_emp_xy, sigma_emp_yy = np.nan, np.nan, np.nan\n",
                "    if all_normals_level and len(all_normals_level) >= 2:\n",
                "        normals_array = np.array(all_normals_level)\n",
                "        if normals_array.ndim == 2 and normals_array.shape[0] > 1 : \n",
                "            cov_matrix_emp = np.cov(normals_array.T) \n",
                "            if cov_matrix_emp.shape == (2,2):\n",
                "                sigma_emp_xx = cov_matrix_emp[0, 0]\n",
                "                sigma_emp_xy = cov_matrix_emp[0, 1]\n",
                "                sigma_emp_yy = cov_matrix_emp[1, 1]\n",
                "\n",
                "    # 4) Compute the area of the sublevel set {Z <= level_val}\n",
                "    volume_sublevel_set = np.sum(bool_image) * gstools_model_rescale_factor**2 # Convert pixel counts to physical area\n",
                "\n",
                "    # Initialize Cabana/Euler placeholders for the current iteration\n",
                "    cos_integral = np.nan\n",
                "    sin_integral = np.nan\n",
                "    total_length_cabana = np.nan\n",
                "    euler_char = np.nan\n",
                "    est_angle_cabana = np.nan\n",
                "    est_kappa_cabana = np.nan\n",
                "    est_kappa_BD = np.nan\n",
                "\n",
                "    # 5) Apply Cabana's formula together with Euler-characteristic constraints\n",
                "    if all_normals_level and points_per_path_actual and \\\n",
                "       sum(points_per_path_actual) == len(all_normals_level) and \\\n",
                "       sum(points_per_path_actual) > 0:\n",
                "        euler_char, cos_integral, sin_integral, total_length_cabana, cell_perimeters, cell_sum_cos_cabana, cell_sum_sin_cabana = calculate_statistics_from_normals(all_normals_level, equispaced_points_level, points_per_path_actual, gstools_model_rescale_factor, factor_divide=factor_divide, field_size=field_size)\n",
                "        # 5.1) Estimate the preferred direction using Cabana's integrals\n",
                "        if not (np.isnan(cos_integral) or np.isnan(sin_integral) or (cos_integral == 0 and sin_integral == 0)):\n",
                "             est_angle_cabana = -(1/2)*np.arctan(sin_integral/cos_integral)+np.pi/2\n",
                "             cabana_statistics = np.sqrt((cos_integral**2 + sin_integral**2) / total_length_cabana**2)\n",
                "             estimated_kappa_cabana = cabana_kappa_estimate(cabana_statistics)\n",
                "        # 6) Estimate κ via the Biermé-Desolneux statistic\n",
                "        if not (np.isnan(euler_char) or np.isnan(perimeter) or (euler_char == 0 and perimeter == 0)):\n",
                "            # Guard against division by zero when the perimeter vanishes\n",
                "            if perimeter > 0:\n",
                "                rescaled_level_estimate = norm.ppf(volume_sublevel_set / (T_parameter**2))\n",
                "                rescaled_perimeter_estimate = (np.sqrt(2 * np.pi)  * perimeter) / (2 * T_parameter**2 * norm.pdf(rescaled_level_estimate))\n",
                "                rescaled_euler_characteristic = (2 * np.pi * euler_char) / (rescaled_level_estimate * norm.pdf(rescaled_level_estimate) * T_parameter**2)\n",
                "                rescaled_euler_characteristic_scikit = (2 * np.pi * euler_characteristic_scikit) / (rescaled_level_estimate * norm.pdf(rescaled_level_estimate) * T_parameter**2)\n",
                "                rescaled_euler_characteristic_scikit2 = (2 * np.pi * euler_characteristic_scikit2) / (rescaled_level_estimate * norm.pdf(rescaled_level_estimate) * T_parameter**2)\n",
                "                # Compute the Biermé-Desolneux summary statistic\n",
                "                # rescaled_euler_characteristic feeds directly into the estimator\n",
                "                # to compute the κ estimate\n",
                "                bierme_desolneux_statistics = np.abs(rescaled_euler_characteristic / (rescaled_perimeter_estimate**2))\n",
                "                bierme_desolneux_statistics_scikit = np.abs(rescaled_euler_characteristic_scikit / (rescaled_perimeter_estimate**2))\n",
                "                bierme_desolneux_statistics_scikit2 = np.abs(rescaled_euler_characteristic_scikit2 / (rescaled_perimeter_estimate**2))\n",
                "                # Convert the statistic into a κ estimate\n",
                "                # rescaled_euler_characteristic feeds directly into the estimator\n",
                "                # to compute the κ estimate\n",
                "                estimated_kappa_BD = bierme_desolneux_kappa_estimate(bierme_desolneux_statistics)\n",
                "                estimated_kappa_BD_scikit = bierme_desolneux_kappa_estimate(bierme_desolneux_statistics_scikit)\n",
                "                estimated_kappa_BD_scikit2 = bierme_desolneux_kappa_estimate(bierme_desolneux_statistics_scikit2)\n",
                "            else:\n",
                "                # Degenerate perimeter => cannot estimate κ\n",
                "                estimated_kappa_BD = np.nan\n",
                "        else:\n",
                "            # Skip the BD estimate if the Euler characteristic is undefined\n",
                "            estimated_kappa_BD = np.nan\n",
                "        # 7) Derive cell-wise Cabana statistics for the χ^2 test\n",
                "        empirical_variance_test_cells = (factor_divide-1) * (np.var(cell_sum_cos_cabana) + np.var(cell_sum_sin_cabana))/2\n",
                "        chi2_statistics = (cos_integral**2 + sin_integral**2) / (factor_divide**2 * empirical_variance_test_cells)\n",
                "        chi2_pvalue = 1 - chi2.cdf(chi2_statistics, df=2)\n",
                "\n",
                "    # Persist the iteration results\n",
                "    new_row_data = {\n",
                "        'kappa': kappa,\n",
                "        'angle_rad': angle_rad,\n",
                "        'a_parameter': a_parameter,\n",
                "        'field_size': field_size,\n",
                "        'unit_length': unit_length,\n",
                "        'T_parameter': T_parameter,\n",
                "        'gstools_model_anis_ratio': gstools_model_anis_ratio,\n",
                "        'gstools_model_rescale_factor': gstools_model_rescale_factor,\n",
                "        'gstools_model_len_scale': gstools_model_len_scale,\n",
                "        'level_val': level_val,\n",
                "        'total_points_generated': sum(points_per_path_actual) if points_per_path_actual else 0,\n",
                "        'sigma_emp_xx': sigma_emp_xx,\n",
                "        'sigma_emp_xy': sigma_emp_xy,\n",
                "        'sigma_emp_yy': sigma_emp_yy,\n",
                "        'estimated_angle_cabana': est_angle_cabana,\n",
                "        'estimated_kappa_cabana': estimated_kappa_cabana,\n",
                "        'empirical_cosine_cabana': cos_integral,\n",
                "        'empirical_sine_cabana': sin_integral,\n",
                "        'euler_characteristic': euler_char,\n",
                "        'perimeter': perimeter,\n",
                "        'volume_sublevel_set': volume_sublevel_set,\n",
                "        'perimeter_cabana': total_length_cabana,\n",
                "        'estimated_kappa_BD': estimated_kappa_BD,\n",
                "        'statistics_BD': bierme_desolneux_statistics if 'bierme_desolneux_statistics' in locals() else np.nan,\n",
                "        'mean_statistics_BD': elliptic_bierme_desolneux(kappa),\n",
                "        'euler_characteristic_scikit_1': euler_characteristic_scikit,\n",
                "        'euler_characteristic_scikit_2': euler_characteristic_scikit2,\n",
                "        'estimated_kappa_BD_scikit_1': estimated_kappa_BD_scikit,\n",
                "        'estimated_kappa_BD_scikit_2': estimated_kappa_BD_scikit2,\n",
                "        'chi2_statistic': chi2_statistics if 'chi2_statistics' in locals() else np.nan,\n",
                "        'chi2_pvalue': chi2_pvalue if 'chi2_pvalue' in locals() else np.nan,\n",
                "        'factor_divide': factor_divide if 'factor_divide' in locals() else np.nan,\n",
                "        'full_covariance_emp_gradients_xx': full_covariance_emp_gradients_xx if 'full_covariance_emp_gradients_xx' in locals() else np.nan,\n",
                "        'full_covariance_emp_gradients_xy': full_covariance_emp_gradients_xy if 'full_covariance_emp_gradients_xy' in locals() else np.nan,\n",
                "        'full_covariance_emp_gradients_yy': full_covariance_emp_gradients_yy if 'full_covariance_emp_gradients_yy' in locals() else np.nan,\n",
                "        'estimated_kappa_gradients': estimated_kappa_gradients if 'estimated_kappa_gradients' in locals() else np.nan,\n",
                "        'estimated_angle_gradients': estimated_angle_gradients if 'estimated_angle_gradients' in locals() else np.nan,\n",
                "        'full_correlation_emp_gradients_xx': full_correlation_emp_gradients_xx if 'full_correlation_emp_gradients_xx' in locals() else np.nan,\n",
                "        'full_correlation_emp_gradients_xy': full_correlation_emp_gradients_xy if 'full_correlation_emp_gradients_xy' in locals() else np.nan,\n",
                "        'full_correlation_emp_gradients_yy': full_correlation_emp_gradients_yy if 'full_correlation_emp_gradients_yy' in locals() else np.nan,\n",
                "        'estimated_kappa_normals': estimated_kappa_normals if 'estimated_kappa_normals' in locals() else np.nan,\n",
                "        'estimated_angle_normals': estimated_angle_normals if 'estimated_angle_normals' in locals() else np.nan\n",
                "    }\n",
                "    new_row = pd.DataFrame([new_row_data])\n",
                "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
                "\n",
                "print(\"\\\\\\\\nSimulation finished.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76baf6b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the aggregated results table\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e5f94ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(3, 1, figsize=(10, 18))\n",
                "\n",
                "# Plot the distribution of Euler characteristic estimates\n",
                "sns.histplot(results_df['euler_characteristic'].dropna(), kde=True, stat=\"density\", bins=20, ax=axes[0])\n",
                "axes[0].set_title('Distribution of Euler Characteristic')\n",
                "axes[0].set_xlabel('Euler Characteristic')\n",
                "axes[0].set_ylabel('Density')\n",
                "axes[0].grid(True)\n",
                "\n",
                "# Plot the distribution of contour perimeters\n",
                "sns.histplot(results_df['perimeter'].dropna(), kde=True, stat=\"density\", bins=20, ax=axes[1])\n",
                "axes[1].set_title('Distribution of Perimeter')\n",
                "axes[1].set_xlabel('Perimeter')\n",
                "axes[1].set_ylabel('Density')\n",
                "axes[1].grid(True)\n",
                "\n",
                "# Plot the distribution of sublevel-set areas\n",
                "sns.histplot(results_df['volume_sublevel_set'].dropna(), kde=True, stat=\"density\", bins=20, ax=axes[2])\n",
                "axes[2].set_title('Distribution of Volume of Sublevel Set')\n",
                "axes[2].set_xlabel('Volume')\n",
                "axes[2].set_ylabel('Density')\n",
                "axes[2].grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7cfc4f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "\n",
                "# Capture the current timestamp\n",
                "current_date = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
                "# Alternative timestamp format example\n",
                "# current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "# Build a unique filename to avoid collisions\n",
                "\n",
                "# Compose the dated filename\n",
                "csv_filename = f'results/simulation_results_{current_date}.csv'\n",
                "\n",
                "# Persist results_df to disk as CSV\n",
                "results_df.to_csv(csv_filename, index=False)\n",
                "print(f\"Results DataFrame saved to {csv_filename}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
